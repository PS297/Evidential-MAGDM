{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32aa4df-cbdd-4584-8a41-6bdcd7ad4798",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e549a81-f97b-48b4-b3f6-5cb991951976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import Augmentor\n",
    "from Augmentor import Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score, recall_score, f1_score, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b127bc4-3c07-43f0-a668-4d98888a338c",
   "metadata": {},
   "source": [
    "## Loading the Augmented training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ece1c7-91ef-446d-b1df-08036289ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Generate Scale Space using Gaussian Smoothing by varying the standard deviation ################\n",
    "def MultiScale(image , sigma):\n",
    "    kernel_size = 3\n",
    "    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n",
    "    return blurred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9a5a1a-1827-4b3c-87b6-4fb949c76570",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 224\n",
    "\n",
    "\n",
    "train_images_S1 = []\n",
    "train_images_S2 = []\n",
    "train_images_S3 =[]\n",
    "train_labels = []\n",
    "\n",
    "for directory_path in glob.glob(\"OCTDL/Augment/train/*\"):\n",
    "    label = directory_path.split(\"/\")[-1]\n",
    "    \n",
    "\n",
    "    for img_path in glob.glob(os.path.join(directory_path , \"*.jpg\")):\n",
    "         gray_img = cv2.imread(img_path , cv2.IMREAD_COLOR)\n",
    "         gray_img = cv2.resize(gray_img , (SIZE , SIZE))\n",
    "         img = gray_img             \n",
    "         img_S1 = MultiScale(img , 1)\n",
    "         img_S2 = MultiScale(img , 2)\n",
    "         img_S3 = MultiScale(img , 3)\n",
    "         train_images_S1.append(img_S1)\n",
    "         train_images_S2.append(img_S2)\n",
    "         train_images_S3.append(img_S3)\n",
    "         train_labels.append(label)\n",
    "\n",
    "    for img_path in glob.glob(os.path.join(directory_path , \"*.png\")):\n",
    "         gray_img = cv2.imread(img_path , cv2.IMREAD_COLOR)\n",
    "         gray_img = cv2.resize(gray_img , (SIZE , SIZE))\n",
    "         img = gray_img                     # np.repeat(gray_img[:, :, np.newaxis], 3, axis=2)\n",
    "         img_S1 = MultiScale(img , 1)\n",
    "         img_S2 = MultiScale(img , 2)\n",
    "         img_S3 = MultiScale(img , 3)\n",
    "         train_images_S1.append(img_S1)\n",
    "         train_images_S2.append(img_S2)\n",
    "         train_images_S3.append(img_S3)\n",
    "         train_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65fbf3-c37c-4b94-b486-c555a5e48c8b",
   "metadata": {},
   "source": [
    "## Loding the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d909d9-0443-4e9d-83bc-42b6855960cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_S1 = []\n",
    "test_images_S2 = []\n",
    "test_images_S3 = []\n",
    "test_labels = []\n",
    "\n",
    "for directory_path in glob.glob(\"OCTDL/val/*\"):\n",
    "    label = directory_path.split(\"/\")[-1]\n",
    "    for img_path in glob.glob(os.path.join(directory_path , \"*.jpg\")):\n",
    "         gray_img = cv2.imread(img_path , cv2.IMREAD_COLOR)\n",
    "         gray_img = cv2.resize(gray_img , (SIZE , SIZE))\n",
    "         img = gray_img                   \n",
    "         img_S1 = MultiScale(img , 1)\n",
    "         img_S2 = MultiScale(img , 2)\n",
    "         img_S3 = MultiScale(img , 3)\n",
    "         test_images_S1.append(img_S1)\n",
    "         test_images_S2.append(img_S2)\n",
    "         test_images_S3.append(img_S3)\n",
    "         test_labels.append(label)   \n",
    "        \n",
    "    for img_path in glob.glob(os.path.join(directory_path , \"*.png\")):\n",
    "         gray_img = cv2.imread(img_path , cv2.IMREAD_COLOR)\n",
    "         gray_img = cv2.resize(gray_img , (SIZE , SIZE))\n",
    "         img = gray_img                    \n",
    "         img_S1 = MultiScale(img , 1)\n",
    "         img_S2 = MultiScale(img , 2)\n",
    "         img_S3 = MultiScale(img , 3)\n",
    "         test_images_S1.append(img_S1)\n",
    "         test_images_S2.append(img_S2)\n",
    "         test_images_S3.append(img_S3)\n",
    "         test_labels.append(label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab583a52-8be0-42ca-b517-87c966ff0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400\n",
      "8400\n",
      "8400\n",
      "417\n",
      "417\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images_S1))\n",
    "print(len(train_images_S2))\n",
    "print(len(train_images_S3))\n",
    "\n",
    "print(len(test_images_S1))\n",
    "print(len(test_images_S2))\n",
    "print(len(test_images_S3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a79c6a-7e95-4aff-b900-26c0e4551e64",
   "metadata": {},
   "source": [
    "## Normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7679bd98-876e-4acd-925e-0fd6dd95e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_S1 = np.array(train_images_S1) / 255.0\n",
    "train_images_S2 = np.array(train_images_S2) / 255.0\n",
    "train_images_S3 = np.array(train_images_S3) / 255.0\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_images_S1 = np.array(test_images_S1) / 255.0\n",
    "test_images_S2 = np.array(test_images_S2) / 255.0\n",
    "test_images_S3 = np.array(test_images_S3) / 255.0\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78bdad-266b-419c-8768-0f1d7e0f7662",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a9348e-da47-4cde-a65b-203c2173a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(test_labels)\n",
    "test_labels_encoded = le.transform(test_labels)\n",
    "\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded = le.transform(train_labels)\n",
    "\n",
    "X_train_S1 , Y_train , X_test_S1 , Y_test = train_images_S1 , train_labels_encoded , test_images_S1 , test_labels_encoded\n",
    "X_train_S2 , X_test_S2 = train_images_S2 , test_images_S2\n",
    "X_train_S3 , X_test_S3 = train_images_S3 , test_images_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd06fcf-a698-47d2-a969-cdf49adf82e2",
   "metadata": {},
   "source": [
    "## Converting the dataset into tesor and loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9934d18f-083b-442d-9ae5-ff845f5e84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "X_train = torch.tensor(X_train_S3, dtype=torch.float32).permute(0, 3, 1, 2)  \n",
    "X_test = torch.tensor(X_test_S3, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(torch.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6b6684-6d35-49c1-9a44-9c5823263a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e669d8-464e-4ed1-a90b-96ff2fede204",
   "metadata": {},
   "source": [
    "## Defining the structure of the EfficientNet-B0 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64dd9285-e82b-4f2e-a3d2-17c8647b7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/Pragya/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "Effi_model = models.efficientnet_b0(weights=True)  # train from scratch\n",
    "Effi_model.classifier[1] = nn.Linear(Effi_model.classifier[1].in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Effi_model = Effi_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Effi_model.parameters(), lr=0.0001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96c5ef-02f8-4ac7-b6ca-1d265df44d3d",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a432c1-c80d-4360-a06c-76c7ab2240dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.7356 Acc: 0.7727 | Val Loss: 0.2521 Acc: 0.9185\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "patience, patience_counter = 20, 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    Effi_model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = Effi_model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    Effi_model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = Effi_model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "    val_loss /= total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/200 | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(Effi_model.state_dict(), \"Effi_S3_Weight.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263fcbf-f515-4293-92d4-91c9931c278a",
   "metadata": {},
   "source": [
    "## Loading the learned models for extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7f161a-c1ef-4e86-bfe7-c5ee75fa9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = model.features   \n",
    "        self.avgpool = model.avgpool     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51521f44-62c2-4db2-b4d5-7f3d4dad7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For Expert u1 ####\n",
    "Effi_model_S1 = models.efficientnet_b0(weights=None) \n",
    "Effi_model_S1.classifier[1] = nn.Linear(Effi_model_S1.classifier[1].in_features, num_classes)\n",
    "\n",
    "Effi_model_S1.load_state_dict(torch.load(\"Effi_S1_Weight.pth\", map_location=\"cpu\"))\n",
    "Effi_model_S1.eval()\n",
    "feature_extractor_model_S1 = FeatureExtractor(Effi_model_S1).to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### For Expert u2 ####\n",
    "Effi_model_S2 = models.efficientnet_b0(weights=None)  # train from scratch or load pretrained\n",
    "Effi_model_S2.classifier[1] = nn.Linear(Effi_model_S2.classifier[1].in_features, num_classes)\n",
    "\n",
    "Effi_model_S2.load_state_dict(torch.load(\"Effi_S2_Weight.pth\", map_location=\"cpu\"))\n",
    "Effi_model_S2.eval()\n",
    "feature_extractor_model_S2 = FeatureExtractor(Effi_model_S2).to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### For Expert u3 ####\n",
    "Effi_model_S3 = models.efficientnet_b0(weights=None)  # train from scratch or load pretrained\n",
    "Effi_model_S3.classifier[1] = nn.Linear(Effi_model_S3.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load trained weights\n",
    "Effi_model_S3.load_state_dict(torch.load(\"Effi_S3_Weight.pth\", map_location=\"cpu\"))\n",
    "Effi_model_S3.eval()\n",
    "feature_extractor_model_S3 = FeatureExtractor(Effi_model_S3).to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108559-4472-424a-a4d2-aeb88da0a405",
   "metadata": {},
   "source": [
    "## Changing the shape of extracted feature for Evidential MAGDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557fd81-0e30-4502-8566-839892aa336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "X_train_S11 = torch.tensor(X_train_S1, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_train_S1 shape:\", X_train_S11.shape)\n",
    "\n",
    "X_train_S22 = torch.tensor(X_train_S2, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_train_S2 shape:\", X_train_S22.shape)\n",
    "\n",
    "X_train_S33 = torch.tensor(X_train_S3, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_train_S3 shape:\", X_train_S33.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c346dac-a339-4008-b37f-f01172f7fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model_S1.eval()\n",
    "feature_extractor_model_S1.eval()\n",
    "with torch.no_grad():\n",
    "    train_features_S1 = feature_extractor_model_S1(X_train_S11)\n",
    "\n",
    "train_features_S1 = train_features_S1.cpu().numpy()\n",
    "print(train_features_S1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc05c6-e0c7-45a0-9a67-5b805c127b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model_S2.eval()\n",
    "feature_extractor_model_S2.eval()\n",
    "with torch.no_grad():\n",
    "    train_features_S2 = feature_extractor_model_S2(X_train_S22)\n",
    "\n",
    "train_features_S2 = train_features_S2.cpu().numpy()\n",
    "print(train_features_S2.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8630451-069e-41b1-b915-1b35d3dbf423",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model_S3.eval()\n",
    "feature_extractor_model_S3.eval()\n",
    "with torch.no_grad():\n",
    "    train_features_S3 = feature_extractor_model_S3(X_train_S11)\n",
    "\n",
    "train_features_S3 = train_features_S3.cpu().numpy()\n",
    "print(train_features_S3.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6b4d3-78ad-4be9-bc56-1ed540041c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Features_S1 = train_features_S1\n",
    "train_Features_S2 = train_features_S2\n",
    "train_Features_S3 = train_features_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06698315-df7a-4f61-a750-247ad3f8a7aa",
   "metadata": {},
   "source": [
    "## Defining Membership Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf5901-bf45-4d34-a2f0-7851727e7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership(feature , partition):\n",
    "    \n",
    "    FeatureMembership = []\n",
    "    b = np.max(feature)\n",
    "    a = np.min(feature)\n",
    "    beta = (b-a)/(partition-1)\n",
    "    \n",
    "    for i in range(partition):\n",
    "         FeaturePart = []\n",
    "         if(i==0):\n",
    "             for j in range(len(feature)):\n",
    "                 if(feature[j]>=a and feature[j]<=b):\n",
    "                     FeaturePart.append(1-(feature[j]-a)/(b-a))\n",
    "                 else:\n",
    "                     FeaturePart.append(0)\n",
    "             FeatureMembership.append(FeaturePart)        \n",
    "         elif(i==partition-1):\n",
    "             for j in range(len(feature)):\n",
    "                 if(feature[j]>=a and feature[j]<=b):\n",
    "                     FeaturePart.append(1-(b-feature[j])/(b-a))\n",
    "                 else:\n",
    "                     FeaturePart.append(0)\n",
    "             FeatureMembership.append(FeaturePart)        \n",
    "        \n",
    "         else:\n",
    "             for j in range(len(feature)):\n",
    "                 if(feature[j]>=a and feature[j]<=a+i*beta):\n",
    "                     FeaturePart.append(1-(a+i*beta-feature[j])/(i*beta))\n",
    "                 elif(feature[j]>=a+i*beta and feature[j]<=b):\n",
    "                     FeaturePart.append(1-((feature[j]-a-i*beta)/(b-a-i*beta)))\n",
    "                 else:\n",
    "                     FeaturePart.append(0)\n",
    "             FeatureMembership.append(FeaturePart)       \n",
    "    return FeatureMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58911992-5862-4050-b3be-c0174fd06f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Alternatives , Num_Features = train_Features_S1.shape\n",
    "print(Num_Alternatives , Num_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4428fd2-9d6f-45da-aae9-4e534f0e9d5f",
   "metadata": {},
   "source": [
    "## Converting the extracted features to MAGDM problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4f98c-b45c-4afd-aefe-f7c447dacfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeaturesSet = np.hstack((train_Features_S1 , train_Features_S2 , train_Features_S3)) \n",
    "print(FeaturesSet.shape)\n",
    "\n",
    "_ , Num_FeaturesSet = FeaturesSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d76cf-043c-43d9-9e77-e7db06b9a5f5",
   "metadata": {},
   "source": [
    "## Calcultaing Ordered Weighted Beleif and Plausability Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14fc07-5326-477c-bc32-ef380ac6b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Calculating Belief and Plausibility of each alternative corresponding to the attribute ########################\n",
    "Row , Column = Num_Alternatives , Num_Features\n",
    "k = 0\n",
    "Partition = 5\n",
    "Array = np.zeros((Num_Alternatives , Column))\n",
    "Belief = np.zeros((Num_Alternatives , Num_FeaturesSet) , dtype = np.float32)\n",
    "Belief_Expert_List = [] ## It is denoating the Belief corresponding to each expert in the list formt containg the corresponding array [[455*5] , [455*5] , ... , [455*5]]\n",
    "weights = np.array([0.833, 0.1392, 0.0233, 0.0039, 0.0006])\n",
    "\n",
    "############################################### Evaluating Belief #########################################################################\n",
    "\n",
    "for i in range(Num_FeaturesSet):  ## It will calculate the memebrship for each features in the FeatreSet\n",
    "    if(~(FeaturesSet[:,i] == 0).all()):\n",
    "          A = (np.array(membership(FeaturesSet[ : , i] , Partition))).T  ## It will return a list of list [[],[],[],[],[]]\n",
    "          Column_Sum = A.sum(axis = 0)\n",
    "          Column_Sum[Column_Sum == 0] = 1\n",
    "          \n",
    "          A_Normalized = A/Column_Sum   ## Normalizing the membership coloumnwise to convert it into BPA\n",
    "          A_Belief = np.sum(A_Normalized * weights , axis = 1)  ## Evaluating Belief with respect to one feature and giving weightage to that \n",
    "\n",
    "        \n",
    "          # Reshape to column vector\n",
    "          A_Belief = A_Belief[: , np.newaxis]\n",
    "          Belief[: , i] = A_Belief.flatten()  ## Saving the belief value for each feature in a new array say Belief\n",
    "    if(i == 0 or i % Column != 0):\n",
    "          Array[: , k] = Belief[: , i]\n",
    "          if(i == Num_FeaturesSet - 1):\n",
    "              Belief_Expert_List.append(Array)\n",
    "    elif(i % Column == 0):\n",
    "        Belief_Expert_List.append(Array)\n",
    "        Array = np.zeros((Num_Alternatives , Column))\n",
    "        k = 0\n",
    "        Array[: , k] = Belief[: , i]\n",
    "        #print(i)\n",
    "    k = k+1\n",
    "    #print(i,k)\n",
    "        \n",
    "#print(Belief.shape)\n",
    "print(len(Belief_Expert_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452f022-9ef7-4a22-a2ea-065736f72aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### Evaluating Plausibility #################################################################\n",
    "\n",
    "Belief_SumAll_Experts = np.sum(Belief_Expert_List , axis = 0)\n",
    "Correct_Sum = Belief_SumAll_Experts\n",
    "Correct_Sum[Correct_Sum == 0] = 1\n",
    "\n",
    "#print(Belief_Expert_List[0][:,0])\n",
    "#print(Belief_SumAll_Experts)\n",
    "\n",
    "Pl_List = []  ## It is denoating the Plausibility corresponding to each expert in the list formt containg the corresponding array [[455*5] , [455*5] , ... , [455*5]]\n",
    "\n",
    "for i in range(len(Belief_Expert_List)): \n",
    "    Exceptional_Array = np.zeros((Num_Alternatives , Column))\n",
    "    for j in range(len(Belief_Expert_List)):\n",
    "        if(j != i):\n",
    "            Exceptional_Array += Belief_Expert_List[j]\n",
    "    Pl_List.append(1 - (Exceptional_Array / Correct_Sum))        \n",
    "#print(Pl_List)    \n",
    "Plausibility = np.concatenate(Pl_List , axis = 1)\n",
    "print(Plausibility.shape)  \n",
    "#print(Pl_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa717fa3-4f12-4426-a24b-36fbc2a680f8",
   "metadata": {},
   "source": [
    "## Computing WPBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a6cc6-0f2a-4f5d-af81-1c3f267feec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Evaluating the WPBl with respect to each expert corresponding to the features ################\n",
    "PBl_List = []\n",
    "\n",
    "PBl_neumerator = (np.array(Belief_Expert_List) + np.array(Pl_List)).tolist()  ## It is the list of array containg belief+plausiblity with respect to the features\n",
    "PBl_denominator1 = [np.sum(i , axis = 1) for i in Belief_Expert_List]\n",
    "PBl_denominator2 = [np.sum(j , axis = 1) for j in Pl_List]\n",
    "PBl_denominator =  [arr1 + arr2 for arr1, arr2 in zip(PBl_denominator1 , PBl_denominator2)] \n",
    "\n",
    "print(len(PBl_neumerator))\n",
    "print(len(PBl_denominator))\n",
    "\n",
    "for array1 , array2 in zip(PBl_neumerator , PBl_denominator):\n",
    "    # Check for zero values in array2\n",
    "    if np.any(array2 == 0):\n",
    "        \n",
    "        result = np.zeros((Row , Column))\n",
    "    else:\n",
    "        \n",
    "        result = array1 / array2[:, np.newaxis] \n",
    "    print(result)    \n",
    "    PBl_List.append(result)\n",
    "    \n",
    "    \n",
    "#print((PBl_List))    \n",
    "PBl = np.concatenate(PBl_List, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f6e4e-785c-4f8c-b407-16569e8615c4",
   "metadata": {},
   "source": [
    "## Computing the Ordered Weighted Belief Divregence Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d4548-1fc8-499b-8c36-17907bf68213",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Calcutaing the divergence between each expert for each alternative by considering all the alternatives #########\n",
    "Num_Experts = len(Belief_Expert_List)\n",
    "Num_Div_Pairs = ((Num_Experts - 1) * Num_Experts) // 2\n",
    "Divergence_Array = np.zeros((Row , Num_Div_Pairs)) ## The second place is denoating the total number of pairs for evaluating the divergence between the experts \n",
    "K = 0\n",
    "\n",
    "def Div(Array1 , Array2):\n",
    "    Array = np.zeros((Num_Alternatives , 1))\n",
    "    Row , Column = Array1.shape\n",
    "    for i in range(Row):\n",
    "        Sum = 0\n",
    "        for j in range(Column):\n",
    "            if(Array1[i][j] != 0 and Array2[i][j] != 0): \n",
    "               Sum += (Array1[i][j] * np.log2((2 * Array1[i][j]) / (Array1[i][j] + Array2[i][j]))) + (Array2[i][j] * np.log2((2 * Array2[i][j]) / (Array1[i][j] + Array2[i][j])))\n",
    "            elif(Array1[i][j] == 0 and Array2[i][j] != 0):\n",
    "               Sum += (Array2[i][j] * np.log2((2 * Array2[i][j]) / (Array1[i][j] + Array2[i][j])))\n",
    "            elif(Array1[i][j] != 0 and Array2[i][j] == 0):\n",
    "               Sum += (Array1[i][j] * np.log2((2 * Array1[i][j]) / (Array1[i][j] + Array2[i][j])))\n",
    "        Array[i][0] = 0.5 * Sum\n",
    "    return Array    \n",
    "        \n",
    "for i in range(len(PBl_List)):\n",
    "    Array1 = PBl_List[i]\n",
    "    for j in range(i+1 , len(PBl_List)):\n",
    "        Array2 = PBl_List[j]\n",
    "        Result = Div(Array1 , Array2)\n",
    "        Divergence_Array[ : , K] = Result.flatten()\n",
    "        K = K+1\n",
    "\n",
    "ColumnAvg_Divergence = np.mean(Divergence_Array , axis = 0)\n",
    "print(ColumnAvg_Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebf67a-e572-41e1-a3c9-df151527c7d3",
   "metadata": {},
   "source": [
    "## Computig Diveregence Measure Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa273017-716f-423a-80a2-05e934703108",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Evaluating the weights for the fusion using the divergence matrix ##############################\n",
    "Divergence_Matrix = np.zeros((Num_Experts , Num_Experts))\n",
    "H = 0\n",
    "l = 0\n",
    "for i in range(Num_Experts):\n",
    "    for j in range(l , Num_Experts):\n",
    "        if(i == j):\n",
    "            Divergence_Matrix[i][j] = 0\n",
    "        else:\n",
    "            Divergence_Matrix[i][j] = ColumnAvg_Divergence[H]\n",
    "            H += 1\n",
    "    l += 1       \n",
    "               \n",
    "print(Divergence_Matrix)\n",
    "Final_Divergence_Matrix = (Divergence_Matrix + Divergence_Matrix.T)\n",
    "print(Final_Divergence_Matrix)\n",
    "\n",
    "Avg_Divergence_Matrix = np.mean(Final_Divergence_Matrix , axis =0)\n",
    "print(Avg_Divergence_Matrix)\n",
    "Support = 1/Avg_Divergence_Matrix\n",
    "\n",
    "Weight_Fusion = Support/np.sum(Support , axis = 0) ## The final weight for fusing the feaures for group decision making\n",
    "print(Weight_Fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e5aca-9e61-41b8-80b1-aea333d9f9d3",
   "metadata": {},
   "source": [
    "## Fusing the attribute infromation using conseusnus evidential weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a1872-a783-47c4-ac6e-9f9402c4cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Final Fused Features for training #################################################################\n",
    "Final_Fused_Features_train = np.zeros((Num_Alternatives , Num_Features)) \n",
    "Final_Fused_Features_train += (train_Features_S1 * Weight_Fusion[0]) + (train_Features_S2 * Weight_Fusion[1]) + (train_Features_S3 * Weight_Fusion[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f103b6c-9cf3-4ccc-baa6-f32901e39d2f",
   "metadata": {},
   "source": [
    "## Defining Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb749a6c-c3dd-47cf-ada1-3d397175791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(n_estimators = 1000 , max_features = 25 , random_state = 42)\n",
    "RandomForest.fit(Final_Fused_Features_train , Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a23da-821b-4a2e-853b-374eb857fa5e",
   "metadata": {},
   "source": [
    "## Passing the final fused infromation to RFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c386a2f-fb9a-4f12-b7e1-d0b3a011c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionRF_train = RandomForest.predict(Final_Fused_Features_train)\n",
    "print(\"Train Accuracy = \", accuracy_score(Y_train , PredictionRF_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1befdc-b557-4a52-98f2-1fae00154941",
   "metadata": {},
   "source": [
    "## Extracting the features for test dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a266383-4516-4615-8c7f-61a52abbfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_S11 = torch.tensor(X_test_S1, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_test_S1 shape:\", X_test_S11.shape)\n",
    "\n",
    "X_test_S22 = torch.tensor(X_test_S2, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_test_S2 shape:\", X_test_S22.shape)\n",
    "\n",
    "X_test_S33 = torch.tensor(X_test_S3, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "print(\"Raw X_test_S3 shape:\", X_test_S33.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caea905-2867-4331-9498-1fe4133ee009",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_features_S1 = feature_extractor_model_S1(X_test_S11)\n",
    "\n",
    "test_features_S1 = test_features_S1.cpu().numpy()\n",
    "print(test_features_S1.shape)  # should be (8400, 1280)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_features_S2 = feature_extractor_model_S2(X_test_S22)\n",
    "\n",
    "test_features_S2 = test_features_S2.cpu().numpy()\n",
    "print(test_features_S2.shape)  # should be (8400, 1280)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_features_S3 = feature_extractor_model_S3(X_test_S33)\n",
    "\n",
    "test_features_S3 = test_features_S3.cpu().numpy()\n",
    "print(test_features_S3.shape)  # should be (8400, 1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6471d5-029c-49db-a3db-5cbd9d33ff1c",
   "metadata": {},
   "source": [
    "## Fusing the extracted features of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bee7cf-7e6e-4d8e-ae4a-04a1ee28713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Final Fused Features for training #################################################################\n",
    "test_Features_S1 = test_features_S1\n",
    "test_Features_S2 = test_features_S2\n",
    "test_Features_S3 = test_features_S3\n",
    "Num_Alternatives_test , Num_Features_test = test_Features_S1.shape\n",
    "\n",
    "Final_Fused_Features_test = np.zeros((Num_Alternatives_test , Num_Features_test)) \n",
    "\n",
    "Final_Fused_Features_test += (test_Features_S1 * Weight_Fusion[0]) + (test_Features_S2 * Weight_Fusion[1]) + (test_Features_S3 * Weight_Fusion[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b317da-8f83-4574-8765-79ff0c527d39",
   "metadata": {},
   "source": [
    "## Using RFC trained model for fused of feature of test dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89af1b7-f285-40cd-8396-87652c265922",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionRF_test = RandomForest.predict(Final_Fused_Features_test)\n",
    "print(\"Test Accuracy = \", accuracy_score(Y_test , PredictionRF_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54981e-e7b8-4874-978f-ebab2646817a",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88242a02-61eb-4f0e-8c2f-ec539be0d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test , PredictionRF_test)\n",
    "cm = multilabel_confusion_matrix(Y_test, PredictionRF_test)\n",
    "\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "precision =[]\n",
    "for i in range(len(cm)):\n",
    "    tp = cm[i][1][1]  # True positives\n",
    "    fn = cm[i][1][0]  # False negatives\n",
    "    tn = cm[i][0][0]  # True negatives\n",
    "    fp = cm[i][0][1]  # False positives\n",
    "    sensitivity.append(tp / (tp + fn))\n",
    "    specificity.append(tn / (tn + fp))\n",
    "    precision.append(tp / (tp + fp))\n",
    "\n",
    "\n",
    "f1 = f1_score(Y_test , PredictionRF_test , average = 'weighted')\n",
    "\n",
    "auc = roc_auc_score(Y_test , RandomForest.predict_proba(Final_Fused_Features_test) , multi_class = 'ovr', average = 'macro')\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Sensitivity (Recall) for each class:\", np.mean(sensitivity))\n",
    "print(\"Specificity for each class:\", np.mean(specificity))\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC for each class:\", auc)\n",
    "print(\"Precision: \" , np.mean(precision))\n",
    "\n",
    "overall_auc = roc_auc_score(Y_test , RandomForest.predict_proba(Final_Fused_Features_test) , multi_class = 'ovr', average = 'macro')\n",
    "\n",
    "print(\"Overall AUC:\", overall_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
